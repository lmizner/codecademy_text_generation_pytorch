{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9885c18-f290-4228-87e4-1a6bbadc9924",
   "metadata": {},
   "source": [
    "# Generating Text with Frankenstein Using PyTorch\n",
    "\n",
    "\n",
    "**Disclaimer**\n",
    "\n",
    "The text generation you will train in this project will be trained exclusively from the ebook [_Frankenstein_](https://www.gutenberg.org/cache/epub/84/pg84-images.html) by Mary Shelley that was made freely available by [The Project Gutenberg](https://gutenberg.org/). Users should be aware that the generated text outputs may contain biases or portray perspectives prevalent at the time of the original writing. As such, please use discretion while interpreting the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9566e-3236-4033-a424-ded679023632",
   "metadata": {},
   "source": [
    "**Setup - Import Libraries**\n",
    "\n",
    "Run the cell below to import the libraries and set the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cadf90b-1865-48c4-9082-69c03c33baa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117459990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.manual_seed(1) # set random seed --do not change!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ec455-bc6a-4074-ba30-08747d75f542",
   "metadata": {},
   "source": [
    "## Task Group 1 - Import Text Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32b20c-8479-4570-bc5e-9d4d11111f3b",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "The text file `'frankenstein.txt'` located in the `'datasets'` directory contains an ebook of [_Frankenstein_](https://www.gutenberg.org/cache/epub/84/pg84-images.html) by Mary Shelley that was made freely available by [The Project Gutenberg](https://gutenberg.org/). We'll use this text exclusively to train our text generation model in this project.\n",
    "\n",
    "Begin by using the `with` and `open` statements to open and read the text file to the variable `frankenstein`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab2e565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('datasets/frankenstein.txt', 'r', encoding = 'utf-8') as f:\n",
    "    frankenstein = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eef4ac",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "**Note**: Due to hardware constraints, we'll only use a small portion of the full text to train our text generation model.\n",
    "\n",
    "The novel begins with a series of letters from one of the main characters that create a framed narrative that foreshadows key themes and providing context for the main story.\n",
    "\n",
    "Let's extract the first letter by slicing the text in `frankenstein` from the character starting at position `1380` (inclusive) to the character ending at position `8230` (exclusive). \n",
    "\n",
    "Save and print the extracted letter to the variable `first_letter_text`. Feel free to read the letter to get a sense of the text that our model will be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2394afc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letter 1\n",
      "\n",
      "_To Mrs. Saville, England._\n",
      "\n",
      "\n",
      "St. Petersburgh, Dec. 11th, 17—.\n",
      "\n",
      "\n",
      "You will rejoice to hear that no disaster has accompanied the\n",
      "commencement of an enterprise which you have regarded with such evil\n",
      "forebodings. I arrived here yesterday, and my first task is to assure\n",
      "my dear sister of my welfare and increasing confidence in the success\n",
      "of my undertaking.\n",
      "\n",
      "I am already far north of London, and as I walk in the streets of\n",
      "Petersburgh, I feel a cold northern breeze play upon my cheeks, which\n",
      "braces my nerves and fills me with delight. Do you understand this\n",
      "feeling? This breeze, which has travelled from the regions towards\n",
      "which I am advancing, gives me a foretaste of those icy climes.\n",
      "Inspirited by this wind of promise, my daydreams become more fervent\n",
      "and vivid. I try in vain to be persuaded that the pole is the seat of\n",
      "frost and desolation; it ever presents itself to my imagination as the\n",
      "region of beauty and delight. There, Margaret, the sun is for ever\n",
      "visible, its broad disk just skirting the horizon and diffusing a\n",
      "perpetual splendour. There—for with your leave, my sister, I will put\n",
      "some trust in preceding navigators—there snow and frost are banished;\n",
      "and, sailing over a calm sea, we may be wafted to a land surpassing in\n",
      "wonders and in beauty every region hitherto discovered on the habitable\n",
      "globe. Its productions and features may be without example, as the\n",
      "phenomena of the heavenly bodies undoubtedly are in those undiscovered\n",
      "solitudes. What may not be expected in a country of eternal light? I\n",
      "may there discover the wondrous power which attracts the needle and may\n",
      "regulate a thousand celestial observations that require only this\n",
      "voyage to render their seeming eccentricities consistent for ever. I\n",
      "shall satiate my ardent curiosity with the sight of a part of the world\n",
      "never before visited, and may tread a land never before imprinted by\n",
      "the foot of man. These are my enticements, and they are sufficient to\n",
      "conquer all fear of danger or death and to induce me to commence this\n",
      "laborious voyage with the joy a child feels when he embarks in a little\n",
      "boat, with his holiday mates, on an expedition of discovery up his\n",
      "native river. But supposing all these conjectures to be false, you\n",
      "cannot contest the inestimable benefit which I shall confer on all\n",
      "mankind, to the last generation, by discovering a passage near the pole\n",
      "to those countries, to reach which at present so many months are\n",
      "requisite; or by ascertaining the secret of the magnet, which, if at\n",
      "all possible, can only be effected by an undertaking such as mine.\n",
      "\n",
      "These reflections have dispelled the agitation with which I began my\n",
      "letter, and I feel my heart glow with an enthusiasm which elevates me\n",
      "to heaven, for nothing contributes so much to tranquillise the mind as\n",
      "a steady purpose—a point on which the soul may fix its intellectual\n",
      "eye. This expedition has been the favourite dream of my early years. I\n",
      "have read with ardour the accounts of the various voyages which have\n",
      "been made in the prospect of arriving at the North Pacific Ocean\n",
      "through the seas which surround the pole. You may remember that a\n",
      "history of all the voyages made for purposes of discovery composed the\n",
      "whole of our good Uncle Thomas’ library. My education was neglected,\n",
      "yet I was passionately fond of reading. These volumes were my study\n",
      "day and night, and my familiarity with them increased that regret which\n",
      "I had felt, as a child, on learning that my father’s dying injunction\n",
      "had forbidden my uncle to allow me to embark in a seafaring life.\n",
      "\n",
      "These visions faded when I perused, for the first time, those poets\n",
      "whose effusions entranced my soul and lifted it to heaven. I also\n",
      "became a poet and for one year lived in a paradise of my own creation;\n",
      "I imagined that I also might obtain a niche in the temple where the\n",
      "names of Homer and Shakespeare are consecrated. You are well\n",
      "acquainted with my failure and how heavily I bore the disappointment.\n",
      "But just at that time I inherited the fortune of my cousin, and my\n",
      "thoughts were turned into the channel of their earlier bent.\n",
      "\n",
      "Six years have passed since I resolved on my present undertaking. I\n",
      "can, even now, remember the hour from which I dedicated myself to this\n",
      "great enterprise. I commenced by inuring my body to hardship. I\n",
      "accompanied the whale-fishers on several expeditions to the North Sea;\n",
      "I voluntarily endured cold, famine, thirst, and want of sleep; I often\n",
      "worked harder than the common sailors during the day and devoted my\n",
      "nights to the study of mathematics, the theory of medicine, and those\n",
      "branches of physical science from which a naval adventurer might derive\n",
      "the greatest practical advantage. Twice I actually hired myself as an\n",
      "under-mate in a Greenland whaler, and acquitted myself to admiration. I\n",
      "must own I felt a little proud when my captain offered me the second\n",
      "dignity in the vessel and entreated me to remain with the greatest\n",
      "earnestness, so valuable did he consider my services.\n",
      "\n",
      "And now, dear Margaret, do I not deserve to accomplish some great purpose?\n",
      "My life might have been passed in ease and luxury, but I preferred glory to\n",
      "every enticement that wealth placed in my path. Oh, that some encouraging\n",
      "voice would answer in the affirmative! My courage and my resolution is\n",
      "firm; but my hopes fluctuate, and my spirits are often depressed. I am\n",
      "about to proceed on a long and difficult voyage, the emergencies of which\n",
      "will demand all my fortitude: I am required not only to raise the spirits\n",
      "of others, but sometimes to sustain my own, when theirs are failing.\n",
      "\n",
      "This is the most favourable period for travelling in Russia. They fly\n",
      "quickly over the snow in their sledges; the motion is pleasant, and, in\n",
      "my opinion, far more agreeable than that of an English stagecoach. The\n",
      "cold is not excessive, if you are wrapped in furs—a dress which I have\n",
      "already adopted, for there is a great difference between walking the\n",
      "deck and remaining seated motionless for hours, when no exercise\n",
      "prevents the blood from actually freezing in your veins. I have no\n",
      "ambition to lose my life on the post-road between St. Petersburgh and\n",
      "Archangel.\n",
      "\n",
      "I shall depart for the latter town in a fortnight or three weeks; and my\n",
      "intention is to hire a ship there, which can easily be done by paying the\n",
      "insurance for the owner, and to engage as many sailors as I think necessary\n",
      "among those who are accustomed to the whale-fishing. I do not intend to\n",
      "sail until the month of June; and when shall I return? Ah, dear sister, how\n",
      "can I answer this question? If I succeed, many, many months, perhaps years,\n",
      "will pass before you and I may meet. If I fail, you will see me again soon,\n",
      "or never.\n",
      "\n",
      "Farewell, my dear, excellent Margaret. Heaven shower down blessings on you,\n",
      "and save me, that I may again and again testify my gratitude for all your\n",
      "love and kindness.\n",
      "\n",
      "Your affectionate brother,\n",
      "\n",
      "R. Walton\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_letter_text = frankenstein[1380:8230]\n",
    "print(first_letter_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d8c62-23e1-42da-b685-e68ad36770fa",
   "metadata": {},
   "source": [
    "## Task Group 2 - Tokenization and Create Vocabulary\n",
    "\n",
    "Next, let's preprocess the text using tokenization to tokenize the text of the first letter into individual **character-based** tokens. \n",
    "\n",
    "We'll also create the vocabulary (and inverse vocabulary) containing the collection of unique tokens mapped to unique token IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b051227a",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Use `list()` to tokenize the text in `first_letter_text` into character-based tokens. Save the tokenized text to the variable `tokenized_text`.\n",
    "\n",
    "Print the number of tokens in the tokenized text of the first letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8437daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6850\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = list(first_letter_text)\n",
    "print(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e0ee9-ccba-4267-9a9e-6b19f0c2f96a",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">How many tokens are in the tokenized text of the first letter?</summary>\n",
    "\n",
    "The first letter in _Frankenstein_ contains 6,850 (character-based) tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37b87a",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Next, let's start creating the vocabulary that maps each token to a unique token ID. \n",
    "\n",
    "Using `tokenized_text`, create a list of unique tokens sorted alphabetically. Save the list to the variable `unique_char_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0e03a1-da7a-4c14-b759-3df6a7c13594",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_char_tokens = sorted(list(set(tokenized_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc9209",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Now, create the vocabulary by assigning each unique token to a token ID based on their positional index in the sorted list `unique_char_tokens`. Save the vocabulary to the variable `c2ix`. \n",
    "\n",
    "Print the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5dd50f9-27d1-463a-8375-5f25b13b1a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, ',': 3, '-': 4, '.': 5, '1': 6, '7': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'B': 12, 'D': 13, 'E': 14, 'F': 15, 'G': 16, 'H': 17, 'I': 18, 'J': 19, 'L': 20, 'M': 21, 'N': 22, 'O': 23, 'P': 24, 'R': 25, 'S': 26, 'T': 27, 'U': 28, 'W': 29, 'Y': 30, '_': 31, 'a': 32, 'b': 33, 'c': 34, 'd': 35, 'e': 36, 'f': 37, 'g': 38, 'h': 39, 'i': 40, 'j': 41, 'k': 42, 'l': 43, 'm': 44, 'n': 45, 'o': 46, 'p': 47, 'q': 48, 'r': 49, 's': 50, 't': 51, 'u': 52, 'v': 53, 'w': 54, 'x': 55, 'y': 56, 'z': 57, '—': 58, '’': 59}\n"
     ]
    }
   ],
   "source": [
    "c2ix = {ch:i for i, ch in enumerate(unique_char_tokens)}\n",
    "print(c2ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43452c7d-9b94-4385-8fc4-7e4bb106e027",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">What does the vocabulary consist of?</summary>\n",
    "\n",
    "The vocabulary consists of special characters (like whitespaces and punctuation marks), numbers, uppercase characters, and lowercase characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5704578",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Let's obtain the vocabulary size (the number of unique tokens). Save the vocabulary size to the variable `vocab_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "541fac44-1793-4c03-acde-b302a97430e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(c2ix)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262a410-fb34-4952-801b-954202da4469",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">What is the size of the vocabulary?</summary>\n",
    "\n",
    "The vocabulary size tells us that there are **60** unique character-based tokens in the vocabulary out of the 6,850 tokens in the tokenized text of the first letter in _Frankenstein_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb2498",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Using the vocabulary `c2ix`, create the inverse vocabulary that maps the token ID back to their text-based token. Save the inverse vocabulary to the variable `ix2c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efabdf2d-82e7-4b6c-957a-7cc3c52165c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix2c = {ix:ch for ch, ix in c2ix.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8b2a5",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Now, let's use the vocabulary `c2ix` to map each token in the tokenized text  to their token IDs. Save the token ID mapping to the variable `tokenized_id_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb81f5c-6b0e-46f0-8ca3-b2a338bdb7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_id_text = [c2ix[ch] for ch in tokenized_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552bd51",
   "metadata": {},
   "source": [
    "## Task Group 3 - Create Sequences for the Features and Labels\n",
    "\n",
    "Since we're training an LSTM to generate text, we'll need to preprocess the tokenized text and create sequences of tokens for the features and their corresponding labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa790b94",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "Start by importing the PyTorch utility classes `Dataset` and `DataLoader` from the `torch.utils.data` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "377ed633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41092102",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Create a class named `TextDataset` that utilizes the `Dataset` class such that:\n",
    "\n",
    "1. The `__init__` method should have the following:\n",
    "    - takes in the variables `tokenized_text` and `seq_length` as inputs \n",
    "    - the attribute `self.tokenized_text` that is assigned with the variable `tokenized_text`\n",
    "    - the attribute `self.seq_length` that is assigned with the variable `seq_length`\n",
    "  \n",
    "2. The `__len__` method that counts the number of features available for training:\n",
    "    - Hint: this method should return the difference between the length of the tokenized text and the sequence length\n",
    "  \n",
    "3. The `__getitem__` method should have the following:\n",
    "    - takes in the input variable `idx` that helps to index the tokenized text\n",
    "    - the variable `features` as a tensor that creates sequences from the tokenized text using the index and sequence length\n",
    "    - the variable `labels` that is created by shifting the features by one token to the right\n",
    "    - the method should return the `features` and `labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "729c2cd4-9d72-4042-9d63-bc8ea5ed58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_text, seq_length):\n",
    "        self.tokenized_text = tokenized_text\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.tokenized_text[idx: idx + self.seq_length])\n",
    "        labels = torch.tensor(self.tokenized_text[idx + 1: idx + self.seq_length + 1])\n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f08a45d",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "\n",
    "Let's define a sequence length of `48` tokens and save the integer to the variable `seq_length`.\n",
    "\n",
    "Now use the `TextDataset` class in the previous task to create and store the dataset of features and labels using the tokenized text `tokenized_id_text` and sequence length `seq_length`. Save the created sequences to the variable `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9595e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 48\n",
    "dataset = TextDataset(tokenized_id_text, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523fa911",
   "metadata": {},
   "source": [
    "### Task 12\n",
    "\n",
    "Next, let's use the `DataLoader` utility class to create an iterable that allows us to train our LSTM using one batch at a time.\n",
    "\n",
    "Let's define a batch size of `36` and save the integer to the variable `batch_size`.\n",
    "\n",
    "Next, use the `DataLoader` class to create the iterable using the following inputs:\n",
    "- `dataset` containing the sequences for the features and labels\n",
    "- `batch_size` specifying the batch size\n",
    "- setting `shuffle=True` to shuffle the sequences to improve training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68135298",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 36\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5925c3f5",
   "metadata": {},
   "source": [
    "## Task Group 4 - Build and Train the LSTM Network\n",
    "\n",
    "Now that our text data has been tokenized and preprocessed, let's start building and training the LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8894a9",
   "metadata": {},
   "source": [
    "### Task 13\n",
    "\n",
    "Start by importing the `torch.nn` module with the alias `nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78b77084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e2390b-5e8f-4365-bdc7-4ad9575488f3",
   "metadata": {},
   "source": [
    "### Task 14\n",
    "\n",
    "Let's now create a class for our LSTM model that will be trained to generate text using character-based tokens.\n",
    "\n",
    "Start by defining a named `CharacterLSTM` that inherits the `nn.Module` (the base class for neural networks using PyTorch) with the following:\n",
    "\n",
    "A. The `__init__` method initializing the following components:\n",
    "- `super(CharacterLSTM, self).__init__()` : used for proper initialization purposes\n",
    "- `self.embedding` : an embedding layer that creates embeddings for each token in the vocabulary specified with `48` embedding dimensions\n",
    "- `self.lstm` : an LSTM layer with inputs that match the embedding dimension and outputs a hidden size of `96`\n",
    "- `self.linear` : a linear layer with inputs that match the hidden size of the LSTM layer and outputs equal to the vocabulary size. Be sure to set `batch_first=True`\n",
    "\n",
    "\n",
    "B. The `forward` method takes in the input `x`, and the hidden/cell states `states`. Use the inputs to define the forward method in the following order:\n",
    "   1. pass the input x through the embedding layer\n",
    "   2. pass the embedding output along with the previous states to the LSTM layer and return the output and the updated states\n",
    "   3. pass the LSTM output to the linear layer\n",
    "   4. reshape the linear layer output \n",
    "   5. return the reshaped output and the updated states\n",
    "\n",
    "C. The `init_state` method that initializes each hidden and cell state for every new batch during training that returns:\n",
    "- `hidden` : a tensor of zeros for the hidden state with the shape `(1, batch_size, 96)`\n",
    "- `cell` : a tensor of zeros for the cell state with the shape `(1, batch_size, 96)`\n",
    "- Note: the shape of the tensors for each state corresponds to `(1, batch_size, hidden_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed240d2b-7aa5-47b3-8bcb-b0c664d6cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CharacterLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim = 48) \n",
    "        self.lstm = nn.LSTM(input_size = 48, hidden_size = 96, batch_first = True)\n",
    "        self.linear = nn.Linear(96, vocab_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        x = self.embedding(x)\n",
    "        out, states = self.lstm(x, states)\n",
    "        out = self.linear(out)\n",
    "        out = out.reshape(-1, out.size(2))\n",
    "        return out, states\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, 96)\n",
    "        cell = torch.zeros(1, batch_size, 96)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3eeec",
   "metadata": {},
   "source": [
    "### Task 15\n",
    "\n",
    "Now, let's create an instance of the `CharacterLSTM` class and save it to the variable `lstm_model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b79c78f2-3893-40cf-89c2-9884785ed521",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = CharacterLSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1630b1",
   "metadata": {},
   "source": [
    "### Task 16\n",
    "\n",
    "Next, let's set up the loss function. \n",
    "\n",
    "Create an instance of the **multiclass cross-entropy loss function** and save it to the variable `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b65e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b7a39",
   "metadata": {},
   "source": [
    "### Task 17\n",
    "\n",
    "Now, let's set up the optimizer. \n",
    "\n",
    "Import the `torch.optim` module with the alias `optim`.\n",
    "\n",
    "Create an instance of the `Adam` optimizer using the parameters from the instantiated model `lstm_model` with a learning rate of `0.015`. Save the optimizer to the variable `optimizer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "796ca526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr = 0.015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5c3e6f-8c9e-47b6-8e4b-dbef20071128",
   "metadata": {},
   "source": [
    "### Task 18\n",
    "\n",
    "With all of the LSTM components built and initialized, let's train the LSTM model to generate text.\n",
    "\n",
    "Create a training loop that trains the network for `5` epochs.\n",
    "\n",
    "Within each epoch, loop through each batch of features and labels in `dataloader` such that at each iteration:\n",
    "\n",
    "1. Reset the gradients\n",
    "2. Reset the hidden and cell states\n",
    "3. Apply the forward pass (that returns the output and updates the states)\n",
    "4. Calculate the loss\n",
    "5. Compute the gradients\n",
    "6. Update the weights and biases\n",
    "\n",
    "Be sure to print out the loss every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d73bd69-c0c0-4838-9173-8ac58343eeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], CELoss: 1.1466\n",
      "Epoch [2/5], CELoss: 0.6431\n",
      "Epoch [3/5], CELoss: 0.4771\n",
      "Epoch [4/5], CELoss: 0.4184\n",
      "Epoch [5/5], CELoss: 0.3901\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for features, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        states = lstm_model.init_state(features.size(0))\n",
    "        outputs, states = lstm_model(features, states)\n",
    "        CEloss = loss(outputs, labels.view(-1))\n",
    "        CEloss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], CELoss: {CEloss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f0ec79-0da0-47a6-9daf-ec0750cd2240",
   "metadata": {},
   "source": [
    "## Task Group 5 - Generate Text\n",
    "\n",
    "Let's now generate text from the trained LSTM!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6669460-2d0f-4307-a6cc-29c35e724fdf",
   "metadata": {},
   "source": [
    "### Task 19\n",
    "\n",
    "Now, create a starting prompt to provide context for the model to generate relevant text from.\n",
    "\n",
    "For example, let's see if the trained model can accurately generate the beginning portion of the first letter:\n",
    "\n",
    "```md\n",
    "You will rejoice to hear that no disaster has accompanied the\n",
    "commencement of an enterprise which you have regarded with such evil\n",
    "forebodings. I arrived here yesterday, and my first task is to assure\n",
    "my dear sister of my welfare and increasing confidence in the success\n",
    "of my undertaking.\n",
    "```\n",
    "\n",
    "Let's use the first five words `\"You will rejoice to hear\"` as the starting prompt. Save the string to the variable `starting_prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "946f6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_prompt = \"You will rejoice to hear\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4129c4-7cee-423d-b4bc-e5b8cd1ba1d4",
   "metadata": {},
   "source": [
    "### Task 20\n",
    "\n",
    "Next, we'll need to tokenize the starting prompt into token IDs. This will allow the model to read the starting prompt, update the hidden and cell states, and generate relevant text.\n",
    "\n",
    "Use the vocabulary `c2ix` to map each character in the starting prompt into token IDs. Save the mapping as a PyTorch tensor containing a list of lists to the variable `tokenized_id_prompt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "229e64c7-b4f2-488a-9e90-9bed1aec0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_id_prompt = torch.tensor([[c2ix[ch] for ch in starting_prompt]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f4da9-395c-4a02-b6ec-3467033ff8cd",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">Here is what the tokenized prompt should look like:</summary>\n",
    "\n",
    "```py\n",
    "tokenized_id_prompt = torch.tensor([[30, 46, 52,  1, 54, 40, 43, 43,  1, 49, 36, 41, 46, 40, 34, 36,  1, 51,\n",
    "                                     46,  1, 39, 36, 32, 49,  1, 51, 39, 32, 51,  1, 45, 46,  1, 35, 40, 50,\n",
    "                                     32, 50, 51, 36, 49,  1, 39, 32, 50,  1, 32, 34, 34, 46, 44, 47, 32, 45,\n",
    "                                     ...\n",
    "                                     ...\n",
    "                                     ...]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e5e0e-6f03-4cca-a8b3-edd5e2d5dc56",
   "metadata": {},
   "source": [
    "### Task 21\n",
    "\n",
    "First, let's set the model to evaluation mode using `.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f71c9589-8900-4241-a1c7-716ed744a821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharacterLSTM(\n",
       "  (embedding): Embedding(60, 48)\n",
       "  (lstm): LSTM(48, 96, batch_first=True)\n",
       "  (linear): Linear(in_features=96, out_features=60, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c398e-dcb2-4620-83d2-f858fff4c615",
   "metadata": {},
   "source": [
    "### Task 22\n",
    "\n",
    "Let's generate the next `500` character-based tokens from the trained LSTM. Save the integer `500` to the variable `num_generated_chars`.\n",
    "\n",
    "Next, let's begin creating the text generation loop. \n",
    "\n",
    "Starting within the `torch.no_grad():` context, initialize the clean hidden and cell states to the variable `states` using the `init_state(1)` method in the LSTM model.\n",
    "\n",
    "Create a `for` loop that generates one character per iteration (using `range(num_generated_chars)`) with the following:\n",
    "\n",
    "1. Input the tokenized prompt through the forward pass to generate the output and updated states\n",
    "2. Use `torch.argmax` to select the token ID with the highest output score\n",
    "3. Use the inverse vocabulary `ix2c` to map the selected token ID to its character-based token\n",
    "4. Append the generated token to the starting prompt\n",
    "5. Prepare the generated token for the next iteration\n",
    "\n",
    "Lastly, print the starting prompt with the newly generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b8e4357-8cdc-4abf-89fc-08e7354d3618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will rejoice to hear that no disaster has accompanied the whale-fishing. I do not intend to\n",
      "sail until that my father’s dying injunction of beauty every region hitherto discovered\n",
      "solitudes. What may not be expected the wondrous power which addination as the\n",
      "whole of our good Uncle Thomas’ library. My education was now\n",
      "adm commence this\n",
      "laborious voyage with the soulade a poet and firm; but my faming. I\n",
      "can, even now, remember that a\n",
      "history of all the voyages which have\n",
      "been made in the prospect of arriving at the again a sead with your led my study\n",
      "day and night, and my familiarity with the soury of eternal ing a\n",
      "perpetual splendour. There—for with your led sister, how\n",
      "can I answer this\n",
      "laborious voyage with the soulade a poet and firm; but my faming. I\n",
      "can, even now, remember that a\n",
      "history of all the voyages which have\n",
      "been made in the prospect of arriving at the North Sea;\n",
      "I voluntarily endured cold, famine, thirst, and want of sleep; which\n",
      "wille, its broad disk just skirting the horizon and difficie again a sead with your led my study\n",
      "day and night, and my familiarity with the soury of eternal ing a\n",
      "perpetual splendour. There—for with your led sister, how\n",
      "can I answer this\n",
      "laborious voyage with the soulade a poet and firm; but my faming. I\n",
      "can, even now, remember that a\n",
      "history of all the voyages which have\n",
      "been made in the prospect of arriving at the North Sea;\n",
      "I voluntarily endured cold, famine, thirst, and want of sleep; which\n",
      "wille, its broad disk just skirting the horizon and difficie\n"
     ]
    }
   ],
   "source": [
    "num_generated_chars = 500\n",
    "\n",
    "with torch.no_grad():\n",
    "    states = lstm_model.init_state(1)\n",
    "    for _ in range(num_generated_chars):\n",
    "        output, states = lstm_model(tokenized_id_prompt, states)\n",
    "        predicted_id = torch.argmax(output[-1, :], dim = -1).item()\n",
    "        predicted_char = ix2c[predicted_id]\n",
    "        starting_prompt += predicted_char\n",
    "        tokenized_id_prompt = torch.tensor([[predicted_id]])\n",
    "\n",
    "print(starting_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf0f95-a7eb-44d1-a515-44f9e147009d",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">How is the generated text from the trained LSTM?</summary>\n",
    "\n",
    "Recall, here is the first paragraph that our text generation model attempted to re-generated given the starting prompt `\"You will rejoice to hear\"`:\n",
    "    \n",
    "```md\n",
    "You will rejoice to hear that no disaster has accompanied the\n",
    "commencement of an enterprise which you have regarded with such evil\n",
    "forebodings. I arrived here yesterday, and my first task is to assure\n",
    "my dear sister of my welfare and increasing confidence in the success\n",
    "of my undertaking.\n",
    "```\n",
    "    \n",
    "The generated text from the character-based LSTM shows promising results. For example, it was able to learn some of the aspects of the underlying text as it was able to correctly generate the first phrase: `\"You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which\"`. \n",
    "\n",
    "Although it starts deviating from the original text, it still maintains some coherency and grammar. The tone and thematic style are still consistent with the original text, and the LSTM even attempts to generate new and novel content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d652b",
   "metadata": {},
   "source": [
    "That's the end of our project on building a text generation model based on Mary Shelley's _Frankenstein_! There's definitely a lot of room for improvement and we encourage you to use your skills to explore different techniques to enhance the language model. \n",
    "\n",
    "Here are some areas for improvement:\n",
    "- use the full text (or gather multiple outside texts)\n",
    "- use a larger embedding size (GPT3 uses a dimension size of ~12,000!)\n",
    "- modify the neural network architecture (add more neurons, layers, activation functions, etc.)\n",
    "- increase the number of epochs for training\n",
    "- test different optimizers and learning rates\n",
    "\n",
    "You might want to consider building and training larger language models on your own device or cloud platform with greater memory.\n",
    "\n",
    "Happy coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b0947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
